{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理论笔记\n",
    "\n",
    "\n",
    "## 监督学习与无监督学习的区别\n",
    "\n",
    "区别是：训练数据是否有标记，有为监督，否则无监。\n",
    "\n",
    "- 监督学习（Supervised Learning）\n",
    "  - 回归regression（连续值变化）【例如，根据已有房价建模，预测房价】\n",
    "  - 分类classification（离散值分类）【例如，根据已有肿瘤形态特征信息与肿瘤是否恶性建模，预测一个给定形态的肿瘤是否恶性】\n",
    "- 无监督学习（Unsupervised Learning）：\n",
    "  - 聚类cluster【例如，鸡尾酒会算法，分离背景音和主要声源音；例如，根据互动而猜测社交网络】\n",
    "\n",
    "## 代价函数（Cost Function）\n",
    "\n",
    "机器学习的目标是通过训练数据不断优化假设函数 $h$，使得相应的代价函数 $J$ 最小。\n",
    "\n",
    "在线性回归(Linear Regression)问题中，我们的假设函数定义为 $h_\\theta(x) $，相应的代价函数定义为 $ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h^{(i)} (x)- y^{(i)})^2 $，目标是找到使得相应的代价函数 $J(\\theta)$ 最小的 $\\theta$。\n",
    "\n",
    "在逻辑回归（Logistic Regression）问题中，若将假设函数 $h_\\theta(x)$ 代入用于线性回归的代价函数形式，得到的函数 $J(\\theta)$ 对于 $\\theta$ 是一个非凸函数（non-convex function），\n",
    "\n",
    "> 所谓凸函数（convex function），目前而言可以二次函数为例\n",
    "\n",
    "从而有许多极值点，在此情况下用梯度下降法得到的解不一定是全局最优解。因此在逻辑回归问题中，代价函数可改变为 $ J(\\theta) = -y(log(h_\\theta(x)))-(1-y)(log(1-h_\\theta(x))) $。这实际上是通过统计学上的**极大似然估计**计算得到的表达式。\n",
    "\n",
    "> - **思考 1：代价函数的定义式，类似于统计学上方差定义式，两者之间是否有什么关系？**\n",
    "> > 例如：方差（Variance）表示样本点偏离样本均值的程度，方差越小，表示该样本点越接近样本均值；而代价（Cost）越小，表示我们的假设越接近真实函数（至少在已有数据覆盖到的范围内）\n",
    "\n",
    "> - **思考 2：考虑到统计与机器学习的关系，还有多少机器学习的概念与统计学概念在定义类似/相关度大？**\n",
    "\n",
    "> - **思考 3：这些形式上的相似，是否意味着内涵上的相似？导致处理方法上的相似？**\n",
    "\n",
    "> - **思考 4：可能涉及的知识点包括 极大似然估计，凸优化**\n",
    "\n",
    "- 代价函数有很多种，这是其中一种\n",
    "- 假设函数 h 是自变量（通常记为 x 或 $x^{(1)}, x^{(2)}, \\cdots$）的函数，而代价函数 J 是 h 中独立于自变量的参数的函数\n",
    "\n",
    "## 梯度下降法（Gradient Descent）\n",
    "\n",
    "> 该方法既适用于线性回归（Linear Regression）问题，又适用于逻辑回归（Logistic Regression）问题。\n",
    "> \n",
    "> 算法形式甚至一致，但由于**假设不同**（想看看 $h_\\theta(x)$ 的定义式），实际上是不同的算法。\n",
    "\n",
    "以单变量线性回归问题为例：\n",
    "\n",
    "假设代价函数 $J$ 是 $ \\theta_0 $ 与 $ \\theta_1 $ 的函数\n",
    "\n",
    "概述：\n",
    "1. 为 $ \\theta_0 $ 与 $ \\theta_1 $ 设置初值\n",
    "2. 反复更新 $ \\theta_0 $ 与 $ \\theta_1 $（每一次都**同时更新两个变量**），直到 $J(\\theta_0, \\theta_1)$ 最小\n",
    "    - $temp0 := \\theta_0 - \\alpha * \\frac{\\partial J}{\\partial \\theta_0}$\n",
    "    - $temp1 := \\theta_1 - \\alpha * \\frac{\\partial J}{\\partial \\theta_1}$\n",
    "    - $ \\theta_0 := temp0 $;\n",
    "    - $\\theta_1 := temp1 $;\n",
    "    - 这里的 $\\alpha$ 被称为「学习速率」，它决定了每次更新 $\\theta_j$ 的大小 \n",
    "\n",
    "\\***补充**：这个偏导数 $\\frac{\\partial J}{\\partial \\theta_j} = \\theta_j - \\alpha \\sum_{i = 1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ 对线性回归问题和逻辑回归问题的表达式均成立\n",
    "\n",
    "在多变量线性回归问题中：\n",
    "\n",
    "例如有如下 3 项特征（$n = 3$），训练样本数为 4（$m = 4$）\n",
    "\n",
    "房屋面积 | 房间数 | 使用年限 | 价格\n",
    "-------------|-----------|-------------|---------\n",
    "80 | 4 | 2 | 5000\n",
    "90 | 5 | 3 | 9000\n",
    "30   | 2 | 10 | 1000\n",
    "10 | 1 | 20 | 800\n",
    "\n",
    "其中，每种特征记为下标 $j$，样本序号记为上标 $(i)$，从而第 3 个样本为 $x^{(3)}$，该样本的第 2 个特征（这里是房间数）为 $x^{(3)}_2$\n",
    "\n",
    "此时的假设函数形式为 $h_\\theta(x) = \\theta_0 + \\theta_1 * x_1 + \\theta_2 * x_2 + \\theta_3 * x_3$\n",
    "\n",
    "为了方便起见，假设还有 1 个特征 $x_0$（$\\forall i, x^{(i)}_0 = 1$），那么参数 $\\theta$ 与自变量 $x$ 均可写成矩阵（列矩阵/向量）形式，即 $h_\\theta(x) = \\theta^{T}x$；如此改写后，代价函数也可以相应改写，同时把 $J$ 视为 $\\theta$ 的函数，而不是 $\\theta_1\\cdots\\theta_n$ 的函数\n",
    "\n",
    "### 【技巧】梯度下降法使用技巧之：特征缩放（Feature Scaling）\n",
    "\n",
    "为了让梯度下降法进行得更快，首先对特征值进行正规化（Normalization），即将所有特征值处以该特征的范围（最大值与最小值之差），以便使得所有特征值取值范围都在 -1 与 +1 之间；通常进行的是均值正规化（Mean Normalization），即（？形式类似或者内涵类似？。。。）统计中的标准化，不过分母的标准差 s 实际只要放入该特征的范围（最大值与最小值之差）即可，这样处理之后，所有特征值取值范围均在 -0.5 与 +0.5 之间（一般在这个范围内，即便不在，也偏离得不远）。\n",
    "\n",
    "### 【技巧】梯度下降法使用技巧之：学习速率（Learning Rate）\n",
    "\n",
    "如果梯度下降法运行正常，结果将收敛，表现为代价函数 $J(\\theta)$ 随 $\\theta$ 下降；当下降到某个时候 $J(\\theta)$ 基本持平不再改变时，结果收敛，这里的 $\\theta$ 就是我们想要的。\n",
    "\n",
    "如果梯度下降法运行结果是代价函数 $J(\\theta)$ 随 $\\theta$ 增大而增大，这表明结果发散。通常这是由于**学习速率 $\\alpha$ 过大**导致的；需要把学习速率减小（但也不能过小，否则可能收敛过慢）。\n",
    "\n",
    "## 【技巧】拟合成功的 2 个技巧：特征选择与多项式回归\n",
    "\n",
    "拟合成功的技巧之一是：根据不同情况（即实际影响输出的因素）选择不同的特征。例如同样对于房价而言，到底需要 2 个参数（例如房子沿街宽度 w、房子纵深 d），还是需要 1 个参数（例如房间面积 area = w \\* d）？恰当选择特征是拟合成功的关键之一。\n",
    "\n",
    "拟合成功的技巧之二是：在需要用非线性方程来拟合数据时，令非线性的幂为下标不同的一次项。例如当判断出数据可能拟合 3 次方程时，例如 $h_\\theta(x) = \\theta_0 + \\theta_1 * x + \\theta_2 * x^{2} + \\theta_3 * x^{3} $ 时，可令 $x_1 = x, x_2 = x^{2}, x_3 = x^{3}$。这样，待拟合函数则成为了线性形式，便可以用已知方法拟合出高次方程的未知参数（待定系数）了。\n",
    "\n",
    "## 正规方程（Normalization Equation）\n",
    "\n",
    "假设每个训练样本均有 $n$ 个特征，则第 i 个样本可由一个向量表示为：$\\begin{bmatrix}x^{(i)}_1\\\\x^{(i)}_2\\\\\\vdots\\\\x^{(i)}_n\\end{bmatrix}$\n",
    "\n",
    "构造矩阵 $X = \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1&x^{(1)}_2&\\cdots&x^{(1)}_n\\\\\n",
    "x^{(2)}_1&x^{(2)}_2&\\cdots&x^{(2)}_n\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "x^{(m)}_1&x^{(m)}_2&\\cdots&x^{(m)}_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "则所求参数 $\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "### 如果 $X^{T}X$ 不可逆怎么办\n",
    "\n",
    "检查 2 项：\n",
    "\n",
    "1. 是否存在线性相关的特征？若是，减少特征\n",
    "2. 特征数(列数)是否比样本数（行数）多？若是，除了考虑减少特征，之后可能介绍的新算法（？。。。）外\n",
    "\n",
    "### 与梯度下降法的对比\n",
    "\n",
    "梯度下降法（Gradient Descent） | 正规方程法（Normal equation）\n",
    "---------------------------|---------------------------\n",
    "需要自行确定学习速率$\\alpha$|**不需要**确定学习速率，但需要计算 $(X^{T}X)^{-1}$\n",
    "需要迭代多次以求出最优解（\\*）|一次计算即可得到最优解\n",
    "在训练样本数较大时计算仍较快|时间复杂度为$O(n^{3})$，当 $n>10^{4}$时基本不考虑\n",
    "\n",
    "\\*注：使代价函数 $J(\\theta)$ 最小的 $\\theta$\n",
    "\n",
    "## 逻辑回归（Logistic Regression）\n",
    "\n",
    "### 线性回归（Linear Regression）？并不总是好用\n",
    "\n",
    "这实际上是一个分类（Classification）算法，用于标签（y）值为离散的情况。\n",
    "\n",
    "线性回归似乎能用于分类问题，例如：\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "0, && x = 1, 2, 3 \\\\\n",
    "1, && x = 4, 5, 6\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "在这种情况下，很容易能拟合出一条直线，例如可能是 $h_\\theta(x) = \\frac{1}{6}x$，然后我们可以约定，使 $h_\\theta(x) \\le 0.5$ 的那些 $x$ 都属于 0 类，剩下的属于 1 类。\n",
    "\n",
    "但线性回归并不总是有效。有一种情况会让我们明白：例如当上述例子增加一个点 (20, 1) 时，拟合出的直线是 $h_\\theta = \\frac{1}{20}x$，但此时使 $h_\\theta(x) \\le 0.5$ 的那些 $x$ 中将有许多点不属于 0 类。\n",
    "\n",
    "那么你也许会注意到我上面使用了「约定」一词，你是不是会问：「那我们改变约定不就可以了吗？」问题是我们希望找到一种尽可能在一类问题中通用的约定。所以在这种情况下，线性回归用来解决分类问题就见得捉襟见肘了\n",
    "\n",
    "### 逻辑回归：使用 S 型函数（Sigmoid function）\n",
    "\n",
    "我们可以使用 [S 型函数（Sigmoid function）](https://en.wikipedia.org/wiki/Sigmoid_function) 作为假设函数来帮助我们分类。该模型为$g(z) = \\frac{1}{1 + e^{-z}}$，其中$z = \\theta^{T}x$，故 $g(z) = g(\\theta^{T}x) = h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^{T}x}}$\n",
    "\n",
    "> 该函数是[逻辑函数（Logistic function）](https://en.wikipedia.org/wiki/Logistic_function)的一个特例，这就是「逻辑回归」（Logistic Regression）名字的由来\n",
    "\n",
    "### 决策边界（Decision Boundary）\n",
    "\n",
    "**决策边界**就是分类边界。\n",
    "\n",
    "函数 $h_\\theta(x) = g(\\theta^{T}x) = P(y = 1|x;\\theta)$ 的意义即\n",
    "\n",
    "> 给定 $x$ 且参数为 $\\theta$ 的条件下，$y = 1$ 的概率\n",
    "\n",
    "假定 $h_\\theta(x) \\ge 0.5 \\Rightarrow y = 1; h_\\theta(x) < 0.5 \\Rightarrow y = 0$，\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "& &z \\ge 0 &\\Rightarrow& g(z) \\ge 0.5 \\\\\n",
    "& \\Leftrightarrow & z = \\theta^{T}x \\ge 0 &\\Rightarrow& g(\\theta^{T}x) = h_\\theta(x) \\ge 0.5 \\\\\n",
    "& \\Leftrightarrow & \\theta^{T}x \\ge 0 &\\Rightarrow& h_\\theta(x) \\ge 0.5 \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "也就是说，区域如何划分（即边界如何选取，即如何分类——我们的目标就是要找到一个合适的分类函数，由得到的点输入，输出该点对应的分类），只与参数 $\\theta$ 有关（与训练数据无关）。一旦 $\\theta$ 定下，则 $h_\\theta(x)$ 的表达式也就跟着定下，决策边界就是 $h_\\theta(x) = 0$ 对应的曲线。\n",
    "\n",
    "### 正则化（Regularization）\n",
    "\n",
    "拟合函数时，有 2 种问题可能出现：\n",
    "\n",
    "1. 欠拟合（Underfit, high bias）：例如用一次函数来拟合明显是符合二次函数分布趋势的点，训练数据集里有许多点无法被假设函数拟合\n",
    "2. 过拟合（Overfit, high variance）：精确通过训练数据集中的每一个点，但这将使假设函数对数据集以外的点预测能力降低\n",
    "\n",
    "其中，解决过拟合问题有 2 种方法：\n",
    "\n",
    "1. 减少特征（人工选择特征；利用算法自动选择）\n",
    "2. **正则化**（向代价函数 $J(\\theta)$ 中添加一些正则化项，通过对正则化项的「惩罚代价」提高，从而减少参数量级/数值，使某些参数几乎为 0，不起作用）\n",
    "\n",
    "但是应该添加哪些项目为正则化项呢？我们通常添加的是 $\\lambda\\sum_{j = 1}^m\\theta_j^{2}$，\n",
    "\n",
    "> **注意 1：正则化项目不包含 $\\theta_0$！**\n",
    "> **注意 2：$\\lambda$ 若过大，反倒可能使假设函数变为欠拟合状态**\n",
    "\n",
    "即让除了 $\\theta_0$ 以外的所有参数都参与正则化过程。\n",
    "\n",
    "> 这就意味着，使用梯度下降法（Gradient Descent）更新 $\\theta_j$ 时，$\\theta_0$ 应独立更新（其余 $\\theta_j$ 包含了正则化项目，则同时更新）\n",
    "\n",
    "正则化方法还有一个好：\n",
    "\n",
    "> 保证正规方程法（Normal Equation）中求逆矩阵的项一定是可逆（非奇异）的。\n",
    "\n",
    "在正规方程法中使用正则化方法时，\n",
    "\n",
    "$$\n",
    "\\theta = (X^{T}X + \\lambda A)^{-1}X^{T}y,\n",
    "$$\n",
    "\n",
    "其中，\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "0&0&0&\\cdots&0\\\\\n",
    "0&1&0&\\cdots&0\\\\\n",
    "0&0&1&\\cdots&0\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "0&0&0&\\cdots&1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "# 编程笔记\n",
    "\n",
    "## Week 02：线性回归（Linear Regression）\n",
    "\n",
    "根据手册（从[这里](https://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex1.zip)下载并解压得到的 `ex1.pdf`）的要求：\n",
    "\n",
    "（以下所谓的「键入命令」云云，均假设读者已打开 octave 并已经切换到练习文件夹下）\n",
    "\n",
    "### 1. 热身练习（Warm Up）\n",
    "\n",
    "手册 P2-3\n",
    "\n",
    "在 `warmUpExercise.m` 文件中输入代码，将 1 个 5\\*5 的单位矩阵赋给 A\n",
    "\n",
    "### 2. 尝试提交（Submit）\n",
    "\n",
    "手册 P2-3\n",
    "\n",
    "键入 `submit` 并根据提示分别输入 email 地址以及唯一身份标识符（即 Generate new token 上方那串字符）以提交作业。如果「热身练习」正确，应见到下述提示：\n",
    "\n",
    "```bash\n",
    "==                    Part Name |     Score | Feedback                                             \n",
    "==                    --------- |     ----- | --------                                             \n",
    "==             Warm-up Exercise |  10 /  10 | Nice work!\n",
    "```\n",
    "\n",
    "### 3. 绘制数据（Plot Data）\n",
    "\n",
    "手册 P4\n",
    "\n",
    "根据手册要求，在 `plotData.m` 中填写相应代码以绘制原始数据\n",
    "\n",
    "### 4. 代价函数（Cost Function）\n",
    "\n",
    "手册 P5-6\n",
    "\n",
    "根据代价函数的定义，在 `computeCost.m` 中写出 J 的表达式。注意几点：\n",
    "\n",
    "- theta/X/y 的维度分别是多少？\n",
    "- 对谁求平方？表达式是什么？是否需要加「.」？\n",
    "- 对谁求和？表达式是什么？是否需要加括弧？\n",
    "\n",
    "### 5. 批量梯度下降法（Batch Gradient Descent）\n",
    "\n",
    "手册 P6 最上方的公式\n",
    "\n",
    "关键是：能不能把代码缩为一句？\n",
    "\n",
    "我的尝试结果是：可以。(YOUR CODE HERE 处的)代码为\n",
    "\n",
    "```octave\n",
    "theta = theta - alpha * 1 / m * [sum((theta'*X' - y') .* X(:,1)'); sum((theta'*X' - y') .* X(:,2)')]; % version 01\n",
    "```\n",
    "\n",
    "version 01 不是最简单的代码，不过总算缩成了一句。以下贴出我的思考过程，如果有更简单的代码欢迎交流：\n",
    "\n",
    "1. 我们的修改对象是 theta，因此等式左侧是 1 个 n * 1 的矩阵（向量）；n 多大，取决于有多少参数 theta[j]，即有多少特征。在当前例子中，n = 2\n",
    "    - 为什么说当前例子中 n = 2？请查看 `ex1.m` 中 `theta = gradientDescent(X, y, theta, alpha, iterations);` 这句代码以上的部分\n",
    "\n",
    "2. 公式中的 alpha 和 1/m 都是数，因此关键是通过向量化 $ \\sum_{i=1}^m (h_{\\theta} (x^{(i)})- y^{(i)})x^{i}_j $ 把该公式的结果打包在 1 个 2 * 1 的矩阵（向量）中\n",
    "\n",
    "3. 以改变 theta[0] 为例：\n",
    "    1. 求和符号中的单位项应为：第 i 个样本点假设值和真实值的差，乘上第 i 个样本点的第 0 个特征的值 $ x^{i}_0 $\n",
    "    2. 有 97 个样本点，因此要求 97 次乘积，想到应该用向量「点」乘（即 Octave 中提供的「两个同维度矩阵中对应元素相乘」的操作，不是数学上的「点乘」，）\n",
    "    3. 求完这些乘积后，再求和，故乘积要在求和号之内\n",
    "    4. 再考虑一下 theta/X/y 的维度\n",
    "\n",
    "4. 改变 theta[1] 也是同样的道理，因此有了上述代码 version 01\n",
    "\n",
    "**问题**在于：\n",
    "\n",
    "> 一旦参数数量达到 10 以上，这样写就显得极其冗长了。    \n",
    "> 在这种情况下，应该如何精简代码？通过在内部加 1 个 for 循环遍历每个 theta[j]？有没有向量化的写法？\n",
    "\n",
    "有向量化的写法，这就是如下代码：\n",
    "\n",
    "```octave\n",
    "theta = theta - alpha * 1 / m * ((theta'*X' - y') * X)'; % version 02\n",
    "```\n",
    "\n",
    "为什么这么写？因为矩阵乘法本身的意义（想看看，如果 $A_{m,s} * B _{s, n} = C_{m,n}，那么 $$c_{i,j}$ 与 $A_{m,s}$ 及 $B _{s, n}$ 中元素的关系是什么？），我们不需要利用求和函数`sum()`。\n",
    "\n",
    "\n",
    "由于乘积中有一项就是对样本点 $x^{(i)}$ 的预测值与其真实值的差 $(h_{\\theta} (x^{(i)})- y^{(i)})$，另一项则是该点在第 j 个特征下的取值。把第 j 个特征下的每一个样本点的差值与特征值相乘，最后把这些乘积相加，得到的那个数，就是第 j 个特征贡献给第 j 个参数 $\\theta_j$ 的改变量。\n",
    "\n",
    "所有样本点的预测值与真实值之差写成矩阵就是 $(\\theta^{T}X^{T} - y^{T})$，它将参与每一个参数的改变，所以，把该项放在**左阵**的位置（1 \\* 97）；\n",
    "\n",
    "而**右阵**的每一列则代表一个**特征量向量 $x_j$**（某一列即代表某一个特征 $x_j$，这一列包含了所有样本点在特征下的取值 $x^{(1)}_j,\\cdots,x^{(i)}_j,\\cdots,x^{(m)_j}$），这个例子中有 2 个特征（n = 2），97 个样本（m = 97），因此 X 是 97 \\* 2 的矩阵。\n",
    "\n",
    "> 这里，可以画个简图，即用 1 \\* m 的行向量（「偏差矩阵」），去乘以 n \\* m 的矩阵（「特征量矩阵」），可能就比较好理解为什么把公共部分（「偏差矩阵」）放在左侧了。\n",
    "\n",
    "注意得到的矩阵是 1 \\* n （n 即特征数） 的，因此在进行矩阵乘法运算后，应该转置，才能参与矩阵减法及矩阵赋值的后续运算（左侧矩阵是 n \\* 1 的）。\n",
    "\n",
    "## Week 03：逻辑回归（Logistic Regression）\n",
    "\n",
    "### opt01. 绘制数据（可选练习）\n",
    "\n",
    "。。。（待补充，ex2.pdf - P3）\n",
    "\n",
    "### submit01. 计算 S 型函数\n",
    "\n",
    "这里涉及到 3 个知识点：\n",
    "\n",
    "1. `size(Matrix)` 将返回 1 个 1 \\* 2 的矩阵，表示矩阵 Matrix 的行数与列数\n",
    "2. `zeros(howBig)` 将返回一个维度为 howBig 的零矩阵。若 howBig 为数字 n，则返回 n \\* n 的零矩阵；若 howBig 为 `[rowNum colNum]` 形式的矩阵，则返回 rowNum \\* colNum 的零矩阵\n",
    "3. **（！最大的坑）**若运算符 operation 的操作对象是矩阵 Matrix 中的每一个元素，只要对 Matrix 施加 operation，同时在 operation 前加「.」即可\n",
    "\n",
    "这意味着，submit01 的答案应为：\n",
    "\n",
    "```octave\n",
    "g = 1 ./ (1 + e.^(-z))\n",
    "```\n",
    "\n",
    "\n",
    "# 参考资料\n",
    "\n",
    "1. [Machine learning on Coursera](https://www.coursera.org/learn/machine-learning), by [Andrew Ng](http://www.andrewng.org/)\n",
    "2. [维基百科·帮助:数学公式](https://zh.wikipedia.org/wiki/Help:%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
