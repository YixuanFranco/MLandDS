{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理论笔记\n",
    "\n",
    "\n",
    "## 监督学习与无监督学习的区别\n",
    "\n",
    "区别是：训练数据是否有标记，有为监督，否则无监。\n",
    "\n",
    "- 监督学习（Supervised Learning）\n",
    "  - 回归regression（连续值变化）【例如，根据已有房价建模，预测房价】\n",
    "  - 分类classification（离散值分类）【例如，根据已有肿瘤形态特征信息与肿瘤是否恶性建模，预测一个给定形态的肿瘤是否恶性】\n",
    "- 无监督学习（Unsupervised Learning）：\n",
    "  - 聚类cluster【例如，鸡尾酒会算法，分离背景音和主要声源音；例如，根据互动而猜测社交网络】\n",
    "\n",
    "## 代价函数（Cost Function）\n",
    "\n",
    "机器学习的目标是通过训练数据不断优化假设函数 $h$，使得相应的代价函数 $J$ 最小。\n",
    "\n",
    "在单变量回归(Univariate Regression)问题中，我们的假设函数定义为 $h_\\theta(x) $，相应的代价函数定义为 $ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h^{(i)} (x)- y^{(i)})^2 $，目标是找到使得相应的代价函数 $J(\\theta)$ 最小的 $\\theta$。\n",
    "\n",
    "> \\***思考1：代价函数的定义式，类似于统计学上方差定义式，两者之间是否有什么关系？**\n",
    "> > 例如：方差（Variance）表示样本点偏离样本均值的程度，方差越小，表示该样本点越接近样本均值；而代价（Cost）越小，表示我们的假设越接近真实函数（至少在已有数据覆盖到的范围内）\n",
    "\n",
    "> \\***思考2：考虑到统计与机器学习的关系，还有多少机器学习的概念与统计学概念在定义类似/相关度大？**\n",
    "\n",
    "> \\***思考3：这些形式上的相似，是否意味着内涵上的相似？导致处理方法上的相似？**\n",
    "\n",
    "- 代价函数有很多种，这是其中一种\n",
    "- 假设函数 h 是自变量（通常记为 x 或 $x^{(1)}, x^{(2)}, \\cdots$）的函数，而代价函数 J 是 h 中独立于自变量的参数的函数\n",
    "\n",
    "## 梯度下降法（Gradient Descent）\n",
    "\n",
    "以单变量线性回归问题为例：\n",
    "\n",
    "假设代价函数 $J$ 是 $ \\theta_0 $ 与 $ \\theta_1 $ 的函数\n",
    "\n",
    "概述：\n",
    "1. 为 $ \\theta_0 $ 与 $ \\theta_1 $ 设置初值\n",
    "2. 反复更新 $ \\theta_0 $ 与 $ \\theta_1 $（每一次都**同时更新两个变量**），直到 $J(\\theta_0, \\theta_1)$ 最小\n",
    "    - $temp0 := \\theta_0 - \\alpha * \\frac{\\partial J}{\\partial \\theta_0}$\n",
    "    - $temp1 := \\theta_1 - \\alpha * \\frac{\\partial J}{\\partial \\theta_1}$\n",
    "    - $ \\theta_0 := temp0 $;\n",
    "    - $\\theta_1 := temp1 $;\n",
    "    - 这里的 alpha 被称为「学习速率」，它决定了每次更新 theta_i 的大小 \n",
    "\n",
    "在多变量线性回归问题中：\n",
    "\n",
    "例如有如下 3 项特征（n = 3），训练样本数为 4（m = 4）\n",
    "\n",
    "房屋面积 | 房间数 | 使用年限 | 价格\n",
    "-------------|-----------|-------------|---------\n",
    "80 | 4 | 2 | 5000\n",
    "90 | 5 | 3 | 9000\n",
    "30   | 2 | 10 | 1000\n",
    "10 | 1 | 20 | 800\n",
    "\n",
    "其中，每种特征记为下标 $j$，样本序号记为上标 $(i)$，从而第 3 个样本为 $x^{(3)}$，该样本的第 2 个特征（这里是房间数）为 $x^{(3)}_2$\n",
    "\n",
    "此时的假设函数形式为 $h_\\theta(x) = \\theta_0 + \\theta_1 * x_1 + \\theta_2 * x_2 + \\theta_3 * x_3$\n",
    "\n",
    "为了方便起见，假设还有 1 个特征 $x_0$（$\\forall i, x^{(i)}_0 = 1$），那么参数 $\\theta$ 与自变量 $x$ 均可写成矩阵（列矩阵/向量）形式，即 $h_\\theta(x) = \\theta^{T}x$；如此改写后，代价函数也可以相应改写，同时把 $J$ 视为 $\\theta$ 的函数，而不是 $\\theta_1\\cdots\\theta_n$ 的函数\n",
    "\n",
    "### 【技巧】梯度下降法使用技巧之：特征缩放（Feature Scaling）\n",
    "\n",
    "为了让梯度下降法进行得更快，首先对特征值进行正规化（Normalization），即将所有特征值处以该特征的范围（最大值与最小值之差），以便使得所有特征值取值范围都在 -1 与 +1 之间；通常进行的是均值正规化（Mean Normalization），即（？形式类似或者内涵类似？。。。）统计中的标准化，不过分母的标准差 s 实际只要放入该特征的范围（最大值与最小值之差）即可，这样处理之后，所有特征值取值范围均在 -0.5 与 +0.5 之间（一般在这个范围内，即便不在，也偏离得不远）。\n",
    "\n",
    "### 【技巧】梯度下降法使用技巧之：学习速率（Learning Rate）\n",
    "\n",
    "如果梯度下降法运行正常，结果将收敛，表现为代价函数 $J(\\theta)$ 随 $\\theta$ 下降；当下降到某个时候 $J(\\theta)$ 基本持平不再改变时，结果收敛，这里的 $\\theta$ 就是我们想要的。\n",
    "\n",
    "如果梯度下降法运行结果是代价函数 $J(\\theta)$ 随 $\\theta$ 增大而增大，这表明结果发散。通常这是由于**学习速率 $\\alpha$ 过大**导致的；需要把学习速率减小（但也不能过小，否则可能收敛过慢）。\n",
    "\n",
    "## 【技巧】拟合成功的 2 个技巧：特征选择与多项式回归\n",
    "\n",
    "拟合成功的技巧之一是：根据不同情况（即实际影响输出的因素）选择不同的特征。例如同样对于房价而言，到底需要 2 个参数（例如房子沿街宽度 w、房子纵深 d），还是需要 1 个参数（例如房间面积 area = w \\* d）？恰当选择特征是拟合成功的关键之一。\n",
    "\n",
    "拟合成功的技巧之二是：在需要用非线性方程来拟合数据时，令非线性的幂为下标不同的一次项。例如当判断出数据可能拟合 3 次方程时，例如 $h_\\theta(x) = \\theta_0 + \\theta_1 * x + \\theta_2 * x^{2} + \\theta_3 * x^{3} $ 时，可令 $x_1 = x, x_2 = x^{2}, x_3 = x^{3}$。这样，待拟合函数则成为了线性形式，便可以用已知方法拟合出高次方程的未知参数（待定系数）了。\n",
    "\n",
    "## 正规方程（Normalization Equation）\n",
    "\n",
    "假设每个训练样本均有 $n$ 个特征，则第 i 个样本可由一个向量表示为：$\\begin{bmatrix}x^{(i)}_1\\\\x^{(i)}_2\\\\\\vdots\\\\x^{(i)}_n\\end{bmatrix}$\n",
    "\n",
    "构造矩阵 $X = \\begin{bmatrix}x^{(1)}_1&x^{(1)}_2&\\cdots&x^{(1)}_n\\\\x^{(2)}_1&x^{(2)}_2&\\cdots&x^{(2)}_n\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\x^{(m)}_1&x^{(m)}_2&\\cdots&x^{(m)}_n\\end{bmatrix}$\n",
    "\n",
    "则所求参数 $\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "### 如果 $X^{T}X$ 不可逆怎么办\n",
    "\n",
    "检查 2 项：\n",
    "\n",
    "1. 是否存在线性相关的特征？若是，减少特征\n",
    "2. 特征数(列数)是否比样本数（行数）多？若是，除了考虑减少特征，之后可能介绍的新算法（？。。。）外\n",
    "\n",
    "### 与梯度下降法的对比\n",
    "\n",
    "梯度下降法（Gradient Descent） | 正规方程法（Normal equation）\n",
    "---------------------------|---------------------------\n",
    "需要自行确定学习速率$\\alpha$|**不需要**确定学习速率，但需要计算 $(X^{T}X)^{-1}$\n",
    "需要迭代多次以求出最优解（\\*）|一次计算即可得到最优解\n",
    "在训练样本数较大时计算仍较快|时间复杂度为$O(n^{3})$，当 $n>10^{4}$时基本不考虑\n",
    "\n",
    "\\*注：使代价函数 $J(\\theta)$ 最小的 $\\theta$\n",
    "\n",
    "# 编程笔记\n",
    "\n",
    "## Week 02：线性回归（Linear Regression）\n",
    "\n",
    "根据手册（从[这里](https://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex1.zip)下载并解压得到的 `ex1.pdf`）的要求：\n",
    "\n",
    "（以下所谓的「键入命令」云云，均假设读者已打开 octave 并已经切换到练习文件夹下）\n",
    "\n",
    "### 1. 热身练习（Warm Up）\n",
    "\n",
    "手册 P2-3\n",
    "\n",
    "在 `warmUpExercise.m` 文件中输入代码，将 1 个 5\\*5 的单位矩阵赋给 A\n",
    "\n",
    "### 2. 尝试提交（Submit）\n",
    "\n",
    "手册 P2-3\n",
    "\n",
    "键入 `submit` 并根据提示分别输入 email 地址以及唯一身份标识符（即 Generate new token 上方那串字符）以提交作业。如果「热身练习」正确，应见到下述提示：\n",
    "\n",
    "```bash\n",
    "==                    Part Name |     Score | Feedback                                             \n",
    "==                    --------- |     ----- | --------                                             \n",
    "==             Warm-up Exercise |  10 /  10 | Nice work!\n",
    "```\n",
    "\n",
    "### 3. 绘制数据（Plot Data）\n",
    "\n",
    "手册 P4\n",
    "\n",
    "根据手册要求，在 `plotData.m` 中填写相应代码以绘制原始数据\n",
    "\n",
    "### 4. 代价函数（Cost Function）\n",
    "\n",
    "手册 P5-6\n",
    "\n",
    "根据代价函数的定义，在 `computeCost.m` 中写出 J 的表达式。注意几点：\n",
    "\n",
    "- theta/X/y 的维度分别是多少？\n",
    "- 对谁求平方？表达式是什么？是否需要加「.」？\n",
    "- 对谁求和？表达式是什么？是否需要加括弧？\n",
    "\n",
    "### 5. 批量梯度下降法（Batch Gradient Descent）\n",
    "\n",
    "手册 P6 最上方的公式\n",
    "\n",
    "关键是：能不能把代码缩为一句？\n",
    "\n",
    "我的尝试结果是：可以。(YOUR CODE HERE 处的)代码为\n",
    "\n",
    "```octave\n",
    "theta = theta - alpha * 1 / m * [sum((theta'*X' - y') .* X(:,1)'); sum((theta'*X' - y') .* X(:,2)')]; % version 01\n",
    "```\n",
    "\n",
    "version 01 不是最简单的代码，不过总算缩成了一句。以下贴出我的思考过程，如果有更简单的代码欢迎交流：\n",
    "\n",
    "1. 我们的修改对象是 theta，因此等式左侧是 1 个 n * 1 的矩阵（向量）；n 多大，取决于有多少参数 theta[j]，即有多少特征。在当前例子中，n = 2\n",
    "    - 为什么说当前例子中 n = 2？请查看 `ex1.m` 中 `theta = gradientDescent(X, y, theta, alpha, iterations);` 这句代码以上的部分\n",
    "\n",
    "2. 公式中的 alpha 和 1/m 都是数，因此关键是通过向量化 $ \\sum_{i=1}^m (h_{\\theta} (x^{(i)})- y^{(i)})x^{i}_j $ 把该公式的结果打包在 1 个 2 * 1 的矩阵（向量）中\n",
    "\n",
    "3. 以改变 theta[0] 为例：\n",
    "    1. 求和符号中的单位项应为：第 i 个样本点假设值和真实值的差，乘上第 i 个样本点的第 0 个特征的值 $ x^{i}_0 $\n",
    "    2. 有 97 个样本点，因此要求 97 次乘积，想到应该用向量「点」乘（即 Octave 中提供的「两个同维度矩阵中对应元素相乘」的操作，不是数学上的「点乘」，）\n",
    "    3. 求完这些乘积后，再求和，故乘积要在求和号之内\n",
    "    4. 再考虑一下 theta/X/y 的维度\n",
    "\n",
    "4. 改变 theta[1] 也是同样的道理，因此有了上述代码 version 01\n",
    "\n",
    "**问题**在于：\n",
    "\n",
    "> 一旦参数数量达到 10 以上，这样写就显得极其冗长了。    \n",
    "> 在这种情况下，应该如何精简代码？通过在内部加 1 个 for 循环遍历每个 theta[j]？有没有向量化的写法？\n",
    "\n",
    "有向量化的写法，这就是如下代码：\n",
    "\n",
    "```octave\n",
    "theta = theta - alpha * 1 / m * ((theta'*X' - y') * X)'; % version 02\n",
    "```\n",
    "\n",
    "为什么这么写？因为矩阵乘法本身的意义（想看看，如果 $A_{m,s} * B _{s, n} = C_{m,n}，那么 $$c_{i,j}$ 与 $A_{m,s}$ 及 $B _{s, n}$ 中元素的关系是什么？），我们不需要利用求和函数`sum()`。\n",
    "\n",
    "\n",
    "由于乘积中有一项就是对样本点 $x^{(i)}$ 的预测值与其真实值的差 $(h_{\\theta} (x^{(i)})- y^{(i)})$，另一项则是该点在第 j 个特征下的取值。把第 j 个特征下的每一个样本点的差值与特征值相乘，最后把这些乘积相加，得到的那个数，就是第 j 个特征贡献给第 j 个参数 $\\theta_j$ 的改变量。\n",
    "\n",
    "所有样本点的预测值与真实值之差写成矩阵就是 $(\\theta^{T}X^{T} - y^{T})$，它将参与每一个参数的改变，所以，把该项放在**左阵**的位置（1 \\* 97）；\n",
    "\n",
    "而**右阵**的每一列则代表一个**特征量向量 $x_j$**（某一列即代表某一个特征 $x_j$，这一列包含了所有样本点在特征下的取值 $x^{(1)}_j,\\cdots,x^{(i)}_j,\\cdots,x^{(m)_j}$），这个例子中有 2 个特征（n = 2），97 个样本（m = 97），因此 X 是 97 \\* 2 的矩阵。\n",
    "\n",
    "> 这里，可以画个简图，即用 1 \\* m 的行向量（「偏差矩阵」），去乘以 n \\* m 的矩阵（「特征量矩阵」），可能就比较好理解为什么把公共部分（「偏差矩阵」）放在左侧了。\n",
    "\n",
    "注意得到的矩阵是 1 \\* n （n 即特征数） 的，因此在进行矩阵乘法运算后，应该转置，才能参与矩阵减法及矩阵赋值的后续运算（左侧矩阵是 n \\* 1 的）。\n",
    "\n",
    "\n",
    "# 参考资料\n",
    "\n",
    "1. [Machine learning on Coursera](https://www.coursera.org/learn/machine-learning), by [Andrew Ng](http://www.andrewng.org/)\n",
    "2. [维基百科·帮助:数学公式](https://zh.wikipedia.org/wiki/Help:%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
