{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 监督学习与无监督学习的区别\n",
    "\n",
    "区别是：训练数据是否有标记，有为监督，否则无监。\n",
    "\n",
    "- 监督学习（Supervised Learning）\n",
    "  - 回归regression（连续值变化）【例如，根据已有房价建模，预测房价】\n",
    "  - 分类classification（离散值分类）【例如，根据已有肿瘤形态特征信息与肿瘤是否恶性建模，预测一个给定形态的肿瘤是否恶性】\n",
    "- 无监督学习（Unsupervised Learning）：\n",
    "  - 聚类cluster【例如，鸡尾酒会算法，分离背景音和主要声源音；例如，根据互动而猜测社交网络】\n",
    "\n",
    "## 代价函数（Cost Function）\n",
    "\n",
    "机器学习的目标是通过训练数据不断优化假设函数 $$ h $$，使得相应的代价函数  $$ J $$ 最小。\n",
    "\n",
    "在单变量回归(Univariate Regression)问题中，我们的假设函数定义为 $$h_\\theta(x) $$，相应的代价函数定义为  $$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h^{(i)} (x)- y^{(i)}i)^2 $$，目标是找到使得相应的代价函数  $$ J(\\theta) $$ 最小的 $$ \\theta $$。\n",
    "\n",
    "> \\***思考1：代价函数的定义式，类似于统计学上方差定义式，两者之间是否有什么关系？**\n",
    "> > 例如：方差（Variance）表示样本点偏离样本均值的程度，方差越小，表示该样本点越接近样本均值；而代价（Cost）越小，表示我们的假设越接近真实函数（至少在已有数据覆盖到的范围内）\n",
    "\n",
    "> \\***思考2：考虑到统计与机器学习的关系，还有多少机器学习的概念与统计学概念在定义类似/相关度大？**\n",
    "\n",
    "> \\***思考3：这些形式上的相似，是否意味着内涵上的相似？导致处理方法上的相似？**\n",
    "\n",
    "- 代价函数有很多种，这是其中一种\n",
    "- 假设函数 $$ h $$ 是自变量（通常记为 $$x$$ 或 $$x^{(1)}$$, $$x^{(2)}$$, ...）的函数，而代价函数 $$ J $$ 是 h 中独立于自变量的参数的函数\n",
    "\n",
    "## 梯度下降法（Gradient Descent）\n",
    "\n",
    "以单变量线性回归问题为例：\n",
    "\n",
    "假设代价函数 $$J$$ 是 $$ \\theta_0 $$ 与 $$ \\theta_1 $$ 的函数\n",
    "\n",
    "概述：\n",
    "1. 为 $$ \\theta_0 $$ 与 $$ \\theta_1 $$ 设置初值\n",
    "2. 反复更新 $$ \\theta_0 $$ 与 $$ \\theta_1 $$（每一次都**同时更新两个变量**），直到 $$J(\\theta_0, \\theta_1)$$ 最小\n",
    "    - $$temp0 := \\theta_0 - \\alpha * \\frac{\\partial J}{\\partial \\theta_0}$$\n",
    "    - $$temp1 := \\theta_1 - \\alpha * \\frac{\\partial J}{\\partial \\theta_1}$$\n",
    "    - $$ \\theta_0 := temp0 $$ ;\n",
    "    - $$\\theta_1 := temp1 $$ ;\n",
    "    - 这里的 $$\\alpha$$ 被称为「学习速率」，它决定了每次更新 $$\\theta_i$$ 的大小 \n",
    "\n",
    "在多变量线性回归问题中：\n",
    "\n",
    "例如有如下 3 项特征（n = 3），训练样本数为 4（m = 4）\n",
    "\n",
    "房屋面积 | 房间数 | 使用年限 | 价格\n",
    "-------------|-----------|-------------|---------\n",
    "80 | 4 | 2 | 5000\n",
    "90 | 5 | 3 | 9000\n",
    "30   | 2 | 10 | 1000\n",
    "10 | 1 | 20 | 800\n",
    "\n",
    "其中，每种特征记为下标 $$j$$，样本序号记为上标 $$(i)$$，从而第 3 个样本为 $$x^{(3)}$$，该样本的第 2 个特征（这里是房间数）为 $$x^{(3)}_2$$\n",
    "\n",
    "此时的假设函数形式为 $$h_\\theta(x) = \\theta_0 + \\theta_1 * x_1 + \\theta_2 * x_2 + \\theta_3 * x_3$$\n",
    "\n",
    "为了方便起见，假设还有 1 个特征 $$x_0$$（$$\\forall i, x^{(i)}_0 = 1$$），那么参数 $$\\theta$$ 与自变量 $$x$$ 均可写成矩阵（列矩阵/向量）形式，即 $$h_\\theta(x) = \\theta^{T}x$$；如此改写后，代价函数也可以相应改写，同时把 $$J$$ 视为 $$\\theta$$ 的函数，而不是 $$\\theta_1\\cdots\\theta_n$$ 的函数\n",
    "\n",
    "### 【技巧】梯度下降法使用技巧之：特征缩放（Feature Scaling）\n",
    "\n",
    "为了让梯度下降法进行得更快，首先对特征值进行正规化（Normalization），即将所有特征值处以该特征的范围（最大值与最小值之差），以便使得所有特征值取值范围都在 -1 与 +1 之间；通常进行的是均值正规化（Mean Normalization），即（？形式类似或者内涵类似？。。。）统计中的标准化，不过分母的标准差 s 实际只要放入该特征的范围（最大值与最小值之差）即可，这样处理之后，所有特征值取值范围均在 -0.5 与 +0.5 之间（一般在这个范围内，即便不在，也偏离得不远）。\n",
    "\n",
    "### 【技巧】梯度下降法使用技巧之：学习速率（Learning Rate）\n",
    "\n",
    "如果梯度下降法运行正常，结果将收敛，表现为代价函数 $$J(\\theta)$$ 随 $$\\theta$$ 下降；当下降到某个时候 $$J(\\theta)$$ 基本持平不再改变时，结果收敛，这里的 $$\\theta$$ 就是我们想要的。\n",
    "\n",
    "如果梯度下降法运行结果是代价函数 $$J(\\theta)$$ 随 $$\\theta$$ 增大而增大，这表明结果发散。通常这是由于**学习速率 $$\\alpha$$ 过大**导致的；需要把学习速率减小（但也不能过小，否则可能收敛过慢）。\n",
    "\n",
    "## 【技巧】拟合成功的 2 个技巧：特征选择与多项式回归\n",
    "\n",
    "拟合成功的技巧之一是：根据不同情况（即实际影响输出的因素）选择不同的特征。例如同样对于房价而言，到底需要 2 个参数（例如房子沿街宽度 w、房子纵深 d），还是需要 1 个参数（例如房间面积 area = w \\* d）？恰当选择特征是拟合成功的关键之一。\n",
    "\n",
    "拟合成功的技巧之二是：在需要用非线性方程来拟合数据时，令非线性的幂为下标不同的一次项。例如当判断出数据可能拟合 3 次方程时，例如 $$h_\\theta(x) = \\theta_0 + \\theta_1 * x + \\theta_2 * x^{2} + \\theta_3 * x^{3} $$ 时，可令 $$x_1 = x, x_2 = x^{2}, x_3 = x^{3}$$。这样，待拟合函数则成为了线性形式，便可以用已知方法拟合出高次方程的未知参数（待定系数）了。\n",
    "\n",
    "## 正规方程（Normalization Equation）\n",
    "\n",
    "假设每个训练样本均有 $$n$$ 个特征，则第 i 个样本可由一个向量表示为：$$\\begin{bmatrix}x^{(i)}_1\\\\x^{(i)}_2\\\\\\vdots\\\\x^{(i)}_n\\end{bmatrix}$$\n",
    "\n",
    "构造矩阵 $$X = \\begin{bmatrix}x^{(1)}_1&x^{(1)}_2&\\cdots&x^{(1)}_n\\\\x^{(2)}_1&x^{(2)}_2&\\cdots&x^{(2)}_n\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\x^{(m)}_1&x^{(m)}_2&\\cdots&x^{(m)}_n\\end{bmatrix}$$\n",
    "\n",
    "则所求参数 $$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "### 如果 $$X^{T}X$$ 不可逆怎么办\n",
    "\n",
    "检查 2 项：\n",
    "\n",
    "1. 是否存在线性相关的特征？若是，减少特征\n",
    "2. 特征数(列数)是否比样本数（行数）多？若是，除了考虑减少特征，之后可能介绍的新算法（？。。。）外\n",
    "\n",
    "### 与梯度下降法的对比\n",
    "\n",
    "梯度下降法（Gradient Descent） | 正规方程法（Normal equation）\n",
    "---------------------------|---------------------------\n",
    "需要自行确定学习速率$$\\alpha$$|**不需要**确定学习速率，但需要计算 $$(X^{T}X)^{-1}$$\n",
    "需要迭代多次以求出最优解（\\*）|一次计算即可得到最优解\n",
    "在训练样本数较大时计算仍较快|时间复杂度为$$O(n^{3})$$，当 $$n>10^{4}$$时基本不考虑\n",
    "\n",
    "\\*注：使代价函数 $$J(\\theta)$$ 最小的 $$\\theta$$\n",
    "\n",
    "# 参考资料\n",
    "\n",
    "1. [Machine learning on Coursera](https://www.coursera.org/learn/machine-learning), by [Andrew Ng](http://www.andrewng.org/)\n",
    "2. [维基百科·帮助:数学公式](https://zh.wikipedia.org/wiki/Help:%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
